name: contoso vm
on: 
  push:
#  workflow_dispatch:

jobs:
  store-front:
    runs-on: ubuntu-latest
    name: A job to say hello
    steps:
      - uses: actions/checkout@v4
      - uses: dorny/paths-filter@v3.0.2
        id: changes 
        with: 
          filters: | 
            store-front:
              - 'store-front/**'
      - name: docker build
        if: steps.changes.outputs.store-front == 'true'
        run: docker build -t ${{ secrets.DOCKERHUB_USERNAME }}/store-front-test store-front
      - run: docker image ls
      - run: docker login -u ${{ secrets.DOCKERHUB_USERNAME }} -p ${{ secrets.DOCKER_PASSWORD }}
      - if: steps.changes.outputs.store-front == 'true'
        run: docker push ${{ secrets.DOCKERHUB_USERNAME }}/store-front-test

  product-service:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4.1.7
      - uses: dorny/paths-filter@v3.0.2
        id: changes 
        with: 
          filters: | 
            product-service:
              - 'product-service/**'
      - name: docker build
        if: steps.changes.outputs.product-service == 'true'
        run: docker build -t ${{ secrets.DOCKERHUB_USERNAME }}/product-service-test2 product-service
      - run: docker image ls
      - run: docker login -u ${{ secrets.DOCKERHUB_USERNAME }} -p ${{ secrets.DOCKER_PASSWORD }}
      - if: steps.changes.outputs.product-service == 'true'
        run: docker push ${{ secrets.DOCKERHUB_USERNAME }}/product-service-test2

  infrastructure-terraform:
    runs-on: self-hosted
    if: github.event_name == 'workflow_dispatch'
    steps:
      - name : checkout
        uses: actions/checkout@v4.1.7

      - name: terraform init
        run: terraform init

      - name: terraform plan
        run: terraform plan

      - name: terraform apply
        run: terraform apply -auto-approve

  configure-infra:
    runs-on: [self-hosted, vm]
    if: github.event_name == 'workflow_dispatch'
    steps:
      - name: checkout
        uses: actions/checkout@v4.1.7

      - name: intsall ansible
        run: sudo apt-get install ansible

      - name: ansible-playbook
        run: ansible-playbook -i inventory playbook.yml

      - name: run kubectl
        run: kubectl get pods


  infra-test:
    runs-on: [self-hosted]
    steps:
      - name: voir si la vm k3s-vm est installée
        id: vm
        run: VBoxManage list vms | Select-String -Pattern k3s-vm

      - name: creer vm avec virtualbox
        if: steps.vm.outputs.vm == 'false'
        #creer vm avec virtualbox avec image ubuntu
        run: | 
          vboxmanage createvm --name k3s-vm --register --ostype Ubuntu_64
          #configurer vm avec bridge
          VBoxManage modifyvm k3s-vm --memory 2048 --cpus 2 --vram 16 --nic1 bridged --bridgeadapter1 "Intel(R) Dual Band Wireless-AC 7260"
          VBoxManage storagectl k3s-vm --name "SATA" --add sata --bootable on
          VBoxManage storageattach k3s-vm --storagectl "SATA" --port 0 --device 0 --type hdd --medium 'C:\Users\mbenj\VirtualBox VMs\Clone de ubuntu24-server\Clone de ubuntu24-server.vdi'
          vboxmanage startvm k3s-vm --type headless
#         configurer vm avec nat
#         VBoxManage modifyvm k3s-vm --memory 2048 --cpus 2 --nic1 nat --vram 16
          
      - name : recuperer ip de la vm 
        run: |
          sleep 120
          $ip=VBoxManage guestproperty enumerate k3s-vm --patterns "/VirtualBox/GuestInfo/Net/0/V4/IP" |  ForEach-Object { ($_ -split "'")[1] }
          
      - name: ssh sur la vm
        run: |  
          VBoxManage guestcontrol k3s-vm copyto C:\Users\mbenj\.ssh\id_rsa.pub --username test --password ${{ secrets.vm }} --target-directory /home/test/.ssh/
          VBoxManage guestcontrol k3s-vm run --username test --password ${{ secrets.vm }} --exe "/bin/sh" -- -c "cat /home/test/.ssh/id_rsa.pub >> /home/test/.ssh/authorized_keys"
       #   ssh-keyscan -H $ip >> ~/.ssh/known_hosts -o StrictHostKeyChecking=no -oke

      - name: voir si le runner est installé
        id: runner
        run: ssh test@$ip 'ls /home/test/actions-runner' || echo "runner not installed" -o StrictHostKeyChecking=accept-new

      - name: install runner github action  
        if: steps.runner.outputs.runner == 'runner not installed'       
        run: |
          ssh test@$ip 'mkdir -p /home/test/actions-runner' 
          ssh test@$ip 'curl -o actions-runner-linux-x64-2.322.0.tar.gz -L https://github.com/actions/runner/releases/download/v2.322.0/actions-runner-linux-x64-2.322.0.tar.gz'
          ssh test@$ip 'tar xzf ./actions-runner-linux-x64-2.322.0.tar.gz' 
          ssh test@$ip './config.sh --url https://github.com/MelkiBenjamin/github-action-test --token BCKZTMHZD7AEG553MMLRUALHWL3C4 --tags vm --unattended' 
          ssh test@$ip 'systemctl start actions-runner.service'
          ssh test@$ip 'systemctl status actions-runner.service'
                  
  k3s-install:
    runs-on: [self-hosted, vm]
    needs: infra-test
    steps:
      - name: install k3s
        run: |
          echo {{ secrets.sudo }} | sudo -S curl -sfL https://get.k3s.io | sh 
      
      - name: kubectl
        run: sudo k3s kubectl get node

      - name: passer sans sudo
        run: |
          mkdir -p $HOME/.kube
          sudo k3s kubectl config view --raw > $HOME/.kube/config
          chmod 600 $HOME/.kube/config
#          sudo install -D -o $(whoami) -g $(whoami) -m 600 /etc/rancher/k3s/k3s.yaml $HOME/.kube/config

      - name: test sans sudo
        run: k3s kubectl get node
          
  monitoring:
    runs-on: [self-hosted, vm]
    needs: contoso
    steps:
      - name: installe helm 
        uses: azure/setup-helm@v4.2.0

      - name: install helm apli monitoring 
#        if: github.event_name == 'workflow_dispatch'
        run: >
          helm repo add prometheus-community https://prometheus-community.github.io/helm-charts &&
          helm upgrade --install my-kube-prometheus-stack prometheus-community/kube-prometheus-stack --version 57.0.3 -n monitoring --create-namespace --timeout=35m --set grafana.adminPassword=${{ secrets.ARGOCD }}
          
  argocd:
    runs-on: [self-hosted, vm]
    needs: k3s-install
    steps:
      - name: create namespace argocd
        run: |
          kubectl get namespace argocd || kubectl create namespace argocd

      - name: deployer argocd
        run: |
          kubectl apply -n argocd -f https://raw.githubusercontent.com/argoproj/argo-cd/stable/manifests/install.yaml

      - name: attendre argocd 
        run: |
          kubectl wait --for=condition=ready pod --all -n argocd --timeout=25m
#          kubectl wait --for=condition=available deployment --all -n argocd --timeout=25m

#      - name: load balancing
#        run: |
#          echo '{"spec": {"type": "LoadBalancer"}}' > patch.json
#          kubectl patch svc argocd-server -n argocd --type merge --patch "$(cat patch.json)

#      - name: port-forward argocd
#        run: kubectl port-forward svc/argocd-server -n argocd 8080:443 &

  contoso:
    runs-on: [self-hosted, vm]
    needs: [argocd]
    steps:
      - name: recuperer fichier contoso.yml avec checkout
        uses: actions/checkout@v4.1.7

      - name: Create Application Contoso with Argocd avec namespace 
        run: |
          kubectl apply -f contoso.yml
