name: build test
on: 
  push:
  workflow_dispatch:

jobs:
  hello_world_job:
    runs-on: ubuntu-latest
    name: A job to say hello
    steps:
      - uses: actions/checkout@v4
      - id: foo
        uses: mudler/localai-github-action@v1
        with:
          model: 'hermes-2-theta-llama-3-8b' # Any from models.localai.io, or from huggingface.com with: "huggingface://<repository>/file"

      - name: crewai install 
        run: |
          curl -LsSf https://astral.sh/uv/install.sh | sh
          uv tool install crewai
          uv tool list
          crewai version
      - run: |
          crewai --help
          crewai create crew test55 --skip_provider
          export OPENAI_API_BASE="http://127.0.0.1:8080"
          export OPENAI_API_KEY="test"
          ls -alh
          ls test55
          crewai create crew --help
      - name: modifier config pour modulaire
        run: |
         cat > config/agents.yaml <<'EOF'
         - name: researcher
           role: "{topic} Senior Data Researcher"
           goal: "Uncover cutting-edge developments in {topic}"
           backstory: >
             You're a seasoned researcher with a knack for uncovering the latest developments in {topic}.
             Known for your ability to find the most relevant information and present it in a clear and concise manner.

         - name: reporting_analyst
           role: "{topic} Reporting Analyst"
           goal: "Create detailed reports based on {topic} data analysis and research findings"
           backstory: >
             You're a meticulous analyst with a keen eye for detail. You're known for your ability to turn complex data into
             clear and concise reports, making it easy for others to understand and act on the information you provide.
          EOF
      - run: |
          cat > config/tasks.yaml <<'EOF'
          - name: research_task
            description: >
              Conduct a thorough research about {topic}.
              Make sure you find any interesting and relevant information given the current year is {current_year}.
            expected_output: >
              A list with 10 bullet points of the most relevant information about {topic}
            agent: researcher

          - name: reporting_task
            description: >
              Write a detailed report on the analysis of {topic} based on the research findings.
            expected_output: >
              A well-structured report summarizing the data analysis on {topic}
            agent: reporting_analyst
           EOF
      - run: |
          cat > crew_test.py <<'EOF'
          import yaml
          from crewai import Crew, Process

          Crew(
            agents=yaml.safe_load(open("config/agents.yaml")),
            tasks=yaml.safe_load(open("config/tasks.yaml")),
            process=Process.sequential,
            verbose=True
          ).kickoff()
          EOF

      - run: sudo cat ./test55/README.md
      - run: sudo cat ./test55/src/test55/main.py
      - run: sudo cat ./test55/src/test55/crew.py
      - run: sudo cat ./test55/src/test55/config/agents.yaml
      - run: sudo cat ./test55/src/test55/config/tasks.yaml
      - run: cd test55 && crewai install
      - run: |
          export OPENAI_API_BASE="http://127.0.0.1:8080/v1"
          export OPENAI_API_KEY="test"
          export OPENAI_MODEL_NAME="openai/hermes-2-theta-llama-3-8b"
          cd test55 && crewai run
      - run: ls -alh
      - run: ls -alh test55
      - run: |
          git clone https://github.com/yoheinakajima/babyagi-2o.git
          echo 'clone fait'
          cd babyagi-2o
          python3 -m venv venv
          source venv/bin/activate
          echo 'env virtuel fait'
          pip install litellm
          echo 'install litelm fait'
          export LITELLM_MODEL="localai/hermes-2-theta-llama-3-8b"
    #      echo "creer un script bash que tu va mettre dans un fichier appeler script-b.sh qui va installer k3s" | python main.py
      - run: |
         sudo curl -sS -o spilot_install.sh https://raw.githubusercontent.com/reid41/shell-pilot/main/spilot_install.sh
      - name: Modifier spilot_install.sh pour contourner la validation IP 
        run: | # Commenter la ligne de validation de l'adresse IP dans la fonction add_server_ip 
           sudo sed -i 's/^if \[\[ \$ip_address =~/# if \[\[ \$ip_address =~/' spilot_install.sh
      - name: install Shell pilot 
        run: |
          (echo "127.0.0.1" && echo "127.0.0.1") | sudo bash spilot_install.sh
      - name: config avec localai 
        run: |
          export TERM=dumb
          sudo s-pilot cmp localai
      - name: config avec llama 3 8b
        run: |
          export TERM=dumb
          sudo s-pilot m hermes-2-theta-llama-3-8b
      - run: export TERM=dumb && s-pilot lm
      - run: export TERM=dumb && s-pilot lc
      - run: ls -alh /usr/local/bin/
      - name: Modifier s-pilot pour contourner la confirmation (non dangereux) - Supprimer les lignes
        run: |
          sudo sed -i '/elif [[ "$DANGER_CMD_FOUND" == false ]]; then/,/fi/ {
           /echo -e "\\\\033\[36mWould you like to execute it? (Yes\/No)\\\\033\[0m"/d
           /read run_answer/d
           /if [[ "$run_answer" =~ ^[Yy](es)?$ ]]; then/d
           /fi/d
          }' /usr/local/bin/s-pilot
      - run: cat /usr/local/bin/s-pilot
      - name: test shell-pilot 
        run: |
          export TERM=dumb
          s-pilot p "bonjour je souhaite utiliser localai avec modèle llama et shell-pilot avec cmd:. Comment utiliser Shell-pilot avec commande avec une action localai dans workflows github action ? repond en français." 
      - run: |
          export TERM=dumb
          (echo "cmd:fais ls\ny" && echo "Yes" && echo "Yes" && echo "Yes" ) |  s-pilot
      - run: |
          pipx install shelloracle
  #    - run: |
    #      echo "LocalAI" | shor config init
      - name: Greet
        id: greet
        run: |
                input="Génére moi un code complet en js et css dans un seul fichier d'une application web microservice de front seulement de site web de magasin de livre qui fonctionne. les livres ainsi que le back seront dans d'autres microsercive que l'on ne s'occupe pas. ce code demander a une page d'accueil avec tout les livre pour les mettre aux panier et une page panier qui resume les livre au panier. l'apli est très simple mais est utilisable. Repond avec uniquement du code donc pas de bonjour de phrase avant le code ou apres ou explication ou commentaire et tout le code est dans le meme fichier"

                # Define the LocalAI API endpoint
                API_URL="http://localhost:8080/chat/completions"

                # Create a JSON payload using jq to handle special characters
                json_payload=$(jq -n --arg input "$input" '{
                model: "'$MODEL_NAME'",
                messages: [
                    {
                    role: "system",
                    content: "You are a developer who provides code functional without additional explanations or comments"
                    },
                    {
                    role: "user",
                    content: $input
                    }
                ]
                }')

                # Send the request to LocalAI
                response=$(curl -s -X POST $API_URL \
                -H "Content-Type: application/json" \
                -d "$json_payload")

                # Extract the result from the response
                result="$(echo $response | jq -r '.choices[0].message.content')"

                 # Print the result
                 echo "Result:"
                 echo "$result" 
                 temp_file=$(mktemp)
                 printf "%s" "$result" > "$temp_file"
                 mv "$temp_file" result.html                            
                # now the output can be consumed with ${{ steps.greet.outputs.message }} by other steps
                    
      - name: test2
        run: cat result.html
